# nano-transformers
Nano-Transformers, a project for Transformer related education and knowledge quick reference purpose.

This project targets at
- help you easily understand Transformers from detailed simple codes
- help you easily write Keras codes, with less complains on TensorFlow
- help you easily get investment from VC if you were working on web3

## 2017 Transformer

[Nano Transformer](doc/transformer.md)

## 2018 GPT-1

[Nano GPT-1](doc/gpt-1.md)

## 2019 GPT-2

[Nano GPT-2](doc/gpt-2.md)

## 2020 GPT-3

[Nano GPT-3](doc/gpt-3.md)

## 2021 InstructGPT

[InstructGPT](instruct-gpt/), not fully done yet. 
- Tried to get `emergent` ability in small model small data. Failed. It can't `reasoning`, but `memory` well.

## References

- [greentfrapp/attention-primer](https://github.com/greentfrapp/attention-primer)
    - It's a TensorFlow 1 implementation. That project inspired **nano-transformers**.
- [sainathadapa/attention-primer-pytorch](https://github.com/sainathadapa/attention-primer-pytorch)
    - It's a PyTorch implementation, forked from previous one.
- [karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)
    - It's a PyTorch implementation of GPT2, a rewrite of [karpathy/minGPT](https://github.com/karpathy/minGPT).
    - [Andrej Karpathy](https://karpathy.ai/) was Tesla AI director in 2017-2022. The man behinds the magic. He is actively developing nanoGPT, we can learn a lot from his detailed optimizations.
- The 3 GPT images are captured from [GPT1,2,3](https://www.linkedin.com/posts/ingliguori_gpt1-gpt2-gpt3-activity-7028774382193774592-xdoj).
- [Attention Free Transformer](https://arxiv.org/abs/2105.14103)
- [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794)